---
layout: post
title: "RDB Loader 0.13.0 released"
title-short: RDB Loader Loader 0.9.0
tags: [redshift, postgres, shred, relational databases, storage]
author: Anton
category: Releases
---

We are thrilled to announce [version 0.13.0][release-0130] of Relational Database Loader, 
our component that lets you load your data into relational storages like Redshift or Postgres.

This release marks the shift of RDB Loader from part of snowplow monorepository to independent component as well as merge with RDB Shredder component.

<!--more-->

In this post, we will cover:

1. [Dedicated repository](/blog/2017/09/05/rdb-loader-0.13.0-released#separate-project)
2. [Single folder load](/blog/2017/09/05/rdb-loader-0.13.0-released#folder)
3. [Dry run](/blog/2017/09/05/rdb-loader-0.13.0-released#dry-run)
4. [Other changes](/blog/2017/09/05/rdb-loader-0.13.0-released#other)
5. [Upgrading](/blog/2017/09/05/rdb-loader-0.13.0-released#upgrading)
6. [Contributing](/blog/2017/09/05/rdb-loader-0.13.0-released#contributing)

<h2 id="separate-project">1. Dedicated repository</h2>

This change is mostly transparent for end-users, but still extremely important to note.
Historically, main [snowplow repository][snowplow-repo] contained all components required to load data into different storage targets.
This approach worked well while AWS Redshift could be considered primary storage, but we're gradually moving towards multiple equal storage targets.
This means that we're starting to publish new independent loader components: first was [Elasticsearch Loader][es-loader-090], now we're adding RDB Loader to this list and many more to come.

Dedicated repository means it is not necessary anymore to wait for full Snowplow release with bug fix or new feature.
It is also much simpler to receive community contributions and track history of releational database component.

Beside of RDB Loader itself, RDB Shredder Spark job now also part of RDB Loader repository. 
They always were tightly coupled components and may share a lot of logic related to relational databases.

<h2 id="folder">2. Single folder load</h2>

Quite often pipeline operator needs to load one particular archived directory. Either after pipeline recovery or for test purposes.
Before 0.13.0 the only way was to copy/move entirely directory from archive to `shredded.good` and launch pipeline without enrich and shred jobs, pretending they're already completed.
But file-moves are slow and error-prone, that's why we're introducing new `--folder` option, which allows you to load exactly one directory using only RDB Loader.

Unfortunately, there's no support of it in EmrEtlRunner and you have to either run it locally using helper script or write Dataflow Runner playbook with base64-encoded config files instead of file paths.

Full invocation example:

{% highlight bash %}
$ java -jar $JARFILE \
  --config $BASE64_CONFIG \
  --target $BASE64_TARGET \
  --resolver $BASE64_RESOLVER \
  --folder s3://com-acme-snowplow/archive/shredded/run=2017-09-05-13-30-22 \
  --logkey s3//com-acme-snowplow/log/rdb-loader/$(uuid) 
{% endhighlight %}

You can find full example script in our recent [discourse post][discourse-r90-alert].

Note that RDB Loader uses AWS [Credential Provider Chain][aws-credentials-chain], which means RDB Loader will use credentials provided at user-level (such as `~/.aws/credentials` file of environment variables) credentials rather than ones in `config.yml`.
This is standard behavior on EMR cluster to avoid making credentials visible in EMR console.

<h2 id="dry-run">3. Dry run</h2>

One more new feature especially useful for recovery is `--dry-run` option.
Using it, operator can discover what load SQL statements are going to be executed in RDMS and check if this is what was expected.
Or these statements can be manually tweaked to perform even more granular load.

Statements will be printed to standard output along with other important debug information.

<h2 id="other">4. Other changes</h2>

As part of usual run, RDB Loader performs data discovery at least twice to make sure S3 provides consistent results and no ghost-data was discovered.
However, this functionality isn't very much on demand when archived directory loading and only slows down whole process.
You can skip it now along with other steps, by adding `--skip consistency_check` option:

{% highlight bash %}
$ java -jar $JARFILE \
  --skip consistency_check \
  --config $BASE64_CONFIG \
  --target $BASE64_TARGET \
  --resolver $BASE64_RESOLVER \
  --folder s3://com-acme-snowplow/archive/shredded/run=2017-09-05-13-30-22 \
  --logkey s3//com-acme-snowplow/log/rdb-loader/$(uuid) 
{% endhighlight %}

Also, one important [bug][issue-3] was fixed. It resulted in excessibe amount of S3 requests due broken caching mechanism and might significantly slowdown data-discovery process.

<h2 id="upgrading">5. Upgrading</h2>

Primary way to run RDB Loader is still Snowplow EmrEtlRunner R90+. You need to update `config.yml`:

{% highlight yaml %}
storage:
  versions:
    rdb_loader: 0.13.0        # WAS 0.12.0
{% endhighlight %}

<h2 id="contributing">6. Contributing</h2>

You can check out the [repository][repo] and the
[open issues](https://github.com/snowplow/snowplow-rdb-loader/issues?utf8=âœ“&q=is%3Aissue%20is%3Aopen%20)
if you'd like to get involved!

[repo]: https://github.com/snowplow/snowplow-rdb-loader

[issue-3]: https://github.com/snowplow/snowplow-rdb-loader/issues/3
[issue-34]: https://github.com/snowplow/snowplow-rdb-loader/issues/34

[snowplow-repo]: https://github.com/snowplow/snowplow
[es-loader-090]: https://snowplowanalytics.com/blog/2017/07/21/elasticsearch-loader-0.9.0-released/

[aws-credentials-chain]: http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default

[discourse-spark-tutorial]: https://discourse.snowplowanalytics.com/t/replacing-amazon-redshift-with-apache-spark-for-event-data-modeling-tutorial/1259
[discourse-r90-alert]: https://discourse.snowplowanalytics.com/t/important-alert-r90-r91-bug-may-result-in-shredded-types-not-loading-into-redshift-after-recovery/1422

